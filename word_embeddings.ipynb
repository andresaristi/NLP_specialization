{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import re\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andresaristi/Documents/ML_projects/NLP_specialization/NLP_specialization\n",
      "/Users/andresaristi/Documents/ML_projects/NLP_specialization/NLP_specialization/data/arxiv/arxiv.csv\n"
     ]
    }
   ],
   "source": [
    "# Defining a class that will help create a corpus of sentences\n",
    "# We tokenize using spaCy\n",
    "\n",
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.filename = filename\n",
    "        self.nlp = spacy.blank('en')\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename,'r') as f:\n",
    "            reader = csv.reader(f,delimiter=',')\n",
    "            for _, abstract in reader:\n",
    "                tokens = [t.text.lower() for t in self.nlp(abstract)]\n",
    "                yield tokens\n",
    "\n",
    "# Getting the path\n",
    "string = IPython.extract_module_locals()[1]['__vsc_ipynb_file__']\n",
    "string = re.search(r'(.*)/[^/]+$',string)[1]\n",
    "print(string)\n",
    "file_path = string+'/data/arxiv/arxiv.csv'\n",
    "print(file_path)\n",
    "\n",
    "# Creating the corpus\n",
    "documents = Corpus(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use gensim to train our embeddings\n",
    "# In this case we use words that appear at least 100 times \n",
    "# We use a window of 5 to make up the context\n",
    "# We use a vector of dimension 100\n",
    "\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec(documents, min_count=100, window=5, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3865436 , -3.4418652 , -0.345966  , -1.345566  , -3.5829475 ,\n",
       "        2.6059368 , -1.9089073 , -6.4584317 ,  2.0622869 ,  0.13571577,\n",
       "       -0.10050068,  4.2325068 , -0.96680254, -1.4777763 , -1.169149  ,\n",
       "       -0.4113086 , -2.9504652 ,  0.5074822 ,  3.2960007 ,  1.4932686 ,\n",
       "        2.71777   , -0.43756047,  2.5949423 , -0.5829279 ,  1.370723  ,\n",
       "       -0.51203275,  2.4064481 ,  0.9346702 ,  0.39199895, -0.18639491,\n",
       "       -0.8793313 ,  0.7610941 ,  1.848235  ,  0.7808373 ,  1.2116618 ,\n",
       "       -0.28394905,  0.44210976,  0.03283614,  0.35691968, -0.19711512,\n",
       "       -0.3822037 ,  0.15944734, -0.741966  , -2.0503888 ,  2.4149497 ,\n",
       "       -0.6902034 ,  0.8068218 ,  0.33982426,  3.0197854 , -3.1405947 ,\n",
       "        1.7638701 ,  1.9645101 , -1.3509152 , -1.9796953 ,  2.8062398 ,\n",
       "       -0.25514004,  0.6073491 ,  2.4232326 , -0.36128837, -2.3922203 ,\n",
       "       -2.6346104 ,  2.1847224 ,  1.1137478 , -2.8458521 , -1.0729017 ,\n",
       "       -0.49751028, -2.2450707 , -2.851338  , -1.8931991 , -2.5712783 ,\n",
       "        1.2237376 ,  0.11072556,  1.1025294 ,  1.2337083 , -2.352772  ,\n",
       "        1.3411468 ,  0.06510434, -0.7631846 ,  1.7998749 ,  0.06581078,\n",
       "        0.32865003, -0.26954603, -1.1909467 ,  1.3004731 , -3.4069974 ,\n",
       "       -1.300645  ,  1.0791564 , -1.8022528 ,  1.7798778 ,  2.7102609 ,\n",
       "       -0.32972196, -0.05190101,  1.4899381 ,  0.06864753, -2.119852  ,\n",
       "        1.5103891 ,  0.8626452 ,  0.6789145 , -0.47936082, -1.8553061 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65520215"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('nmt','smt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simultaneous', 0.6187008023262024),\n",
       " ('deep', 0.6168339848518372),\n",
       " ('reinforcement', 0.5470598340034485),\n",
       " ('active', 0.533389687538147),\n",
       " ('back', 0.5300593376159668),\n",
       " ('curriculum', 0.5259509086608887),\n",
       " ('multitask', 0.47756215929985046),\n",
       " ('machines', 0.4620964527130127),\n",
       " ('transfer', 0.4359015226364136),\n",
       " ('st', 0.43239301443099976)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('machine',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic', 0.8391634225845337),\n",
       " ('bayes', 0.8087450265884399),\n",
       " ('svm', 0.8063234686851501),\n",
       " ('naive', 0.7365142107009888),\n",
       " ('forest', 0.7045769095420837),\n",
       " ('binary', 0.646622359752655),\n",
       " ('random', 0.618570864200592),\n",
       " ('kernel', 0.6091437339782715),\n",
       " ('classifier', 0.5828105211257935),\n",
       " ('bilstm', 0.5825992226600647)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/arxiv/arxiv.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/data/arxiv/arxiv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
